% Template for SGN-3507 papers; to be used with:
%          spconf.sty   - ICASSP/ICIP LaTeX style file
%          IEEEtran.bst - IEEE bibliography style file

% Originally created for MCSP 20xx papers;
% Created:  Sep 2006 - Riku Uusikartano -- riku.uusikartano@tut.fi
% Modified: Sep 2007 - Riku Uusikartano -- riku.uusikartano@tut.fi
% Modified: Oct 2009 - Riku Uusikartano -- riku.uusikartano@tut.fi
% Modified: Oct 2010 - Jukka Suhonen -- jukka.suhonen@tut.fi
% Modified: Oct 2011 - Francescantonio Della Rosa -- francescantonio.dellarosa@tut.fi
% Modified: Nov 2011 - Jukka Suhonen -- juka.suhonen@tut.fi
% Modified for SGN-3507 March 2013 - Juha Pajula -- juha.pajula(a)tut.fi
% Modified for SGN-16006 Jan 2014 - Hanna Silen -- hanna.silen(a)tut.fi

% --------------------------------------------------------------------------
\documentclass{article}

% The amsmath and epsfig packages greatly simplify the process of adding
% equations and figures to the document, and thus their use is highly
% recommended.
% ------------
\usepackage{spconf,amsmath,epsfig}
\usepackage{subcaption}

% Title.
% ------
\title{Smile Recognition}

\name{Jesper Granat, Peter Todorov
%\thanks{Insert sponsor acknowledgments (where necessary) here.}
}
%
\address{jesper.granat@tuni.fi, peter.todorov@tuni.fi}

% Hyphenation (hyphenate all names and non-english words here).
% -------------------------------------------------------------
\hyphenation{Tam-pe-re micro-soft}

\begin{document}

\maketitle
\sloppy

% Abstract.
% ---------
\begin{abstract}
In this study, a convolutional neural network was trained to recognize
smile from an image. The training and deployment was done using the
Keras framework on top of the Tensorflow framework. The training data
was 4 000 images of smiling and non-smiling faces. The accuracy on
this dataset was 85.36 \%.
\end{abstract}

% First section, often named as Introduction.
% -------------------------------------------
\section{Introduction}\label{sec:intro}
Since their breakthrough in the 2012 ImageNet
competition~\cite{krizhevsky_2017}, Convolutional Neural Networks
(CNNs) have been widely used in solving various image processing
problems, such as classification~\cite{krizhevsky_2017}, object
recognition~\cite{yolov3} and image enhancement~\cite{wang_2019}. In
this study, a custom design CNN is trained for real-time smile
recognition. Real-time in this context refers to inference speed where
the network can be run on live video feed in over 20 fps on a laptop
with GPU\@.

\section{Background}\label{sec:background}
Convolutional Neural Networks (CNNs) have been shown to be essential
in modern computer vision applications, such as classification.
Classification in particular is rather difficult problem due to its
complexity. Here, the goal is to assign a label from a predefined set
of labels to an image. Difficulties in this task include truncation of
objects (object is partially not in the image), occlusion of objects
(some other object is blocking some parts of the desired object),
variations in color and lighting, and intra-class variation. The
complexity of classification can be intuitively explained by
considering a case where an image of a cat is desired to be
classified. The problem here is learning a representation of a cat, as
many cats can look different, an image of the same cat from different
angle and lighting yields in a completely different matrix of pixel
values in the image.~\cite{cs231n}

Traditional approaches to image classification have utilized
hand-crafted algorithms such Histogram of Oriented Gradients
(HOG)~\cite{hogs}. In such algorithms, pixel color values and pixel
proximities are utilized to create a representation of the training
data. Since 2012~\cite{krizhevsky_2017}, CNNs have been dominant in
classification. Particularly, deep CNNs from 2015~\cite{he_2015} are
still dominant in classification or as backbones for more difficult
tasks, such as image retrieval~\cite{teichmann_2018}. These can
include hundreds or even thousands of layers of convolutions. Typical
CNNs for classification include two types of layers: convolutional
layers and pooling layers. Pooling layers introduce non-linearity to
the model to make is feasible to include several layers of
convolutions. Convolutional layers, on the other hand, are responsible
for creating the image representation. The last layer(s) in CNNs are
in most cases fully connected layers. The output of the network are
the probabilities of each class being in the image.~\cite{cs231n}

CNNs for classification are trained with cross-entropy loss~\cite[Ch.
  6]{goodfellow_2016}. The idea of the loss is to punish weights that
are responsible for wrong classification, and reward weights that are
responsible for correct classification during training. Additionally,
batch normalization~\cite{ioffe_2015} is used to generalize the
results. Furthermore, classification with CNNs includes many more
bells and whistles that create the absolute best results.

\section{Solution}\label{sec:solution}
% 1. & 2.
The CNN model used in this study will be trained on GENKI-4K Face
dataset~\cite{GENKI-4K}. In addition to the smile status, the training
data contains also the face orientation, but that will be ignored in
this study. It could be utilized to orient the faces upright as a
preprocessing step, but that is out of scope of this study. Therefore,
the problem to be solved in this study is formulated as a binary
classification problem. The data is split randomly into 80 \% training
data and 20 \% test data in a stratified manner. This means that both
splits contain equal proportions of positive and negative examples.

% 3.
The CNN architecture used in this study is similar to the one proposed
in the laboratory instructions, and it is implemented with the
Keras~\cite{keras} framework. In addition to the layers proposed by
the instructor, a batch normalization layer is added in front of every
convolutional layer. A $3 \times 3$ convolutional kernel is used in
every convolutional layer. Normally distributed Glorot
initializer~\cite{glorot10a} is used to initialize the kernel weights,
and each layer contains $L2$-regularization~\cite[Ch.
  7]{goodfellow_2016}. The activation function for the fully connected
layer is Sigmoid activation~\cite[Ch. 6]{goodfellow_2016}. The model
is trained using the Adam optimizer~\cite{adam_optimizer}.

% 4.
Finally, an additional sanity check and demonstrator is constructed by
reading a web camera feed in real-time. Each frame is sent to the
model synchronously, and the output of the model is displayed in the
grafical user interface in a human-readable form. This functionality
is implemented with the OpenCV computer vision library~\cite{opencv}.
The project was implemented in Python~\cite{rossum_1995}.

\begin{table}[t]
  \begin{minipage}[t]{0.5\textwidth}\centering
    \centering
    \caption{The confusion matrix for the optimized default architecture
      with batch normalization (accuracy 80.00 \%)}\label{tab:default-confusion}
    \begin{tabular}{lll}
      \hline
      & Not smiling & Smiling \\ \hline
      \multicolumn{1}{l|}{Not smiling} & 287         & 81      \\
      \multicolumn{1}{l|}{Smiling}     & 79          & 353     \\ \hline
    \end{tabular}
  \end{minipage}
  \begin{minipage}[t]{0.5\textwidth}\centering
    \centering
    \caption{The confusion matrix for the optimized 4 layer architecture
      with batch normalization and regularization (accuracy 85.36
      \%)}\label{tab:best-confusion}
    \begin{tabular}{lll}
      \hline
      & Not smiling & Smiling \\ \hline
      \multicolumn{1}{l|}{Not smiling} & 301         & 67     \\
      \multicolumn{1}{l|}{Smiling}     & 50          & 382     \\ \hline
    \end{tabular}
  \end{minipage}
\end{table}

\begin{figure}[t]
    \begin{minipage}[t]{0.5\textwidth}\centering
      \includegraphics[width=\linewidth]{figures/default_bn.pdf}
      \caption{Learning curve with optimized hyperparameters and batch
        normalization with the default architecture}\label{fig:default}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}\centering
      \includegraphics[width=\linewidth]{figures/best.pdf}
      \caption{Learning curve with optimized hyperparameters and batch
        normalization with one extra layer and
        $L2$-regularization}\label{fig:best}
  \end{minipage}
\end{figure}

\section{Results and Discussion}\label{sec:results}

The solution described in Section~\ref{sec:solution} was achieved
through iterative optimization. First, the default parameters were
used, which gave a somewhat reasonable result. Batch normalization was
then added, because from our previous experience with neural networks
we knew it would help. It did improve the results by roughly 5 \%. The
largest difference in this case, however, was achieved by carefully
selecting the loss function. Cross-entropy loss was noted to be the
superior loss for this case. Finally, when $L2$-regularization and one
extra layer of a convolution and max pooling was added, the model
exceeded the desired accuracy of 85 \%. It should be noted that the
best accuracy of those parameters was 86.00 \% at 44 epochs, which is
slightly higher than the final accuracy of 85.36 \% at 50 epochs. Even
though randomness is used in the experiments, the random seeds were
fixed so that the results are both replicatable and comparable. The
batch size was 64 in both cases, and learning rate was set to 0.001.

Table~\ref{tab:default-confusion} shows the confusion matrix for the
optimized default architecture with batch normalization. The learning
curve for the same process is shown in Figure~\ref{fig:default}.
Respectively, Table~\ref{tab:best-confusion} and Figure~\ref{fig:best}
illustrate the same measurements for the final model. As can be seen
from the confusion matrices, both models are quite good at
classification, however, the final model results in sginificantly
better accuracy. The one extra layer adds more expression power to the
model so that it can learn a more complicated representation for the
task. On the other hand, the $L2$-regularization prevents the model
from overfitting.

\begin{figure*}[t]
  \begin{minipage}[t]{0.5\textwidth}\centering
    \begin{subfigure}[t]{0.5\linewidth}\centering
      \includegraphics[width=0.95\linewidth]{figures/example1.jpg}
      \caption{True positive}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\linewidth}\centering
      \includegraphics[width=0.95\linewidth]{figures/example2.jpg}
      \caption{True negative}
    \end{subfigure}
    \caption{Qualitative samples from the test data}\label{fig:tptn}
  \end{minipage}%
  \begin{minipage}[t]{0.5\textwidth}\centering
    \begin{subfigure}[t]{0.5\linewidth}\centering
      \includegraphics[width=0.95\linewidth]{figures/camera_example1.jpg}
      \caption{True positive}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\linewidth}\centering
      \includegraphics[width=0.95\linewidth]{figures/camera_example2.jpg}
      \caption{True negative}
    \end{subfigure}
    \caption{Qualitative samples from the real-time
      functionality}\label{fig:realtime}
  \end{minipage}
\end{figure*}

Qualitative analysis shows that the model works somewhat well. Some
false positives or false negatives on the test data stem from
difficult examples, for example when the person is smirking in the
image. This makes it difficult even for humans to classify the images.
A true negative and a true positive are shown in
Figure~\ref{fig:tptn}.

The real-time part of the program works quite well in situations where
the person is close enough to the camera. This could be improved in
the future by adding a face detector, which would draw a bounding box
around the face, after which the input image could be cropped. Also,
the lighting plays a part in the working of the real-time
functionality. Again, a true negative and a true positive are shown in
Figure~\ref{fig:realtime}. It is important to set the webcam aspect
ratio to one since all training data was square shaped. It is also
important to change the input to match the original training input, so
here $64 \times 64$ is used.


\section{Conclusion}\label{sec:conclusion}
A model for smile classification was trained in this work. The final
model consists of four convolutional, max pooling and batch
normalization layers that were initialized with Glorot-initialization
algorithm. It was trained with Adam optimizer with learning rate of
0.001 (from the original paper), the loss was cross-entropy loss, and
batch size 64. The convolutional kernel size was set to $3 \times 3$.
It achieves accuracy of 85.37 \%.

\small

% IEEEtran is a LaTeX style file defining the reference formatting.
% -----------------------------------------------------------------
\bibliographystyle{IEEEtran}

% IEEEabrv is a LaTeX style file defining the abbreviations of different
% journals and conferences. mcsp_refs contains the actual reference data
% from which the references are selected into the paper using \cite{}.
% ----------------------------------------------------------------------

% Comment of following line if bibliography is not needed
\bibliography{mcsp_refs}

% ---------------------------------------------------------------------------
\vfill\pagebreak

\end{document}

